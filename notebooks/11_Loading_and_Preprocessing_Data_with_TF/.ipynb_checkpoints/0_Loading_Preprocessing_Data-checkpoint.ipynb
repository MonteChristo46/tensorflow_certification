{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e7bfb0-3bcd-476c-94ea-96ef40052e88",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3bf74-3277-4c0f-be8d-a87c489459a0",
   "metadata": {},
   "source": [
    "TensorFlow provides an Data-API to to create dataset objects. TensorFlow takes care of: \n",
    "\n",
    "1. Multithreading\n",
    "2. Queing\n",
    "3. Batching\n",
    "4. Prefetching\n",
    "\n",
    "The TensorFlow DataAPI can read multiple file format: \n",
    "\n",
    "1. CSV\n",
    "2. JSON\n",
    "3. TFRecord\n",
    "4. SQL Databases\n",
    "\n",
    "__Usualy Datasets are used to gradually read data from disk__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "696ac0f2-993a-4355-8451-e93e12f01123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a719f3ca-d11f-498c-8d69-96404f77d1fe",
   "metadata": {},
   "source": [
    "## Important Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39208491-09ad-4081-b71f-0f16bb00aeca",
   "metadata": {},
   "source": [
    "### Creating a Dataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2cc8d1-e5e6-420e-884f-b90ef30b5091",
   "metadata": {},
   "source": [
    "### ```.from_tensor_slices()```\n",
    "\n",
    "Used to create a Dataset which contain the sliced tensors. Tensors are slices across their first dimension: \n",
    "\n",
    "1. __From 1-Dimensional Array__  \n",
    "\n",
    "```python\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "list(dataset.as_numpy_iterator())\n",
    "# dataset = [1,2,3]\n",
    "```\n",
    "\n",
    "2. __From 2-Dimensional Array__  \n",
    "\n",
    "```python\n",
    "dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\n",
    "list(dataset.as_numpy_iterator())\n",
    "# [[1,2], [1,2]]\n",
    "```\n",
    "\n",
    "3. __From Tuples (ATTENTION)__  \n",
    "\n",
    "```python\n",
    "dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\n",
    "list(dataset.as_numpy_iterator())\n",
    "# [(1,3,5), (2,4,6)]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a0f47e6-f1dd-4f5e-8bc4-1977694358db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1a33c85-ab23-453a-acca-54279677e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08b286cc-9f09-45d5-a44d-ff0e16cecbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1bbf7-7859-474a-aa44-79b7e5acb786",
   "metadata": {},
   "source": [
    "### Chaining Transformation\n",
    "\n",
    "Once a dataset is created we can apply multiple transformation methods - e.g.:\n",
    "\n",
    "- ```.batch()```\n",
    "- ```.repeat()```\n",
    "- ```.map()```\n",
    "- ```.apply()```\n",
    "- ```.filter()```\n",
    "- ```.shuffle()```\n",
    "- ```.take()```\n",
    "- ```.list_files()```\n",
    "\n",
    "__Each transformation method returns a new dataset object - so they can be changed together!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90539523-b13f-49cc-b215-698fc4c0781a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None,), types: tf.int32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.batch(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "339a0df2-21b3-4caa-8a81-63c172d8b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = dataset.repeat(3).batch(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ea34077-526b-4915-badd-e09986e7d2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset_:\n",
    "    print(item) # The last Tensor will have a different shape size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85c41908-d295-4905-8a7c-a2e922372b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Turning on drop_remainder\n",
    "for item in dataset.repeat(3).batch(7, drop_remainder=True):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5d0cf31-ef46-48f2-9877-df5b31bb713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Only taking few examples from dataset\n",
    "for item in dataset_.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c4a66-a8d4-4b47-901d-cfcf1c4e0d4f",
   "metadata": {},
   "source": [
    "## Shuffling Data & Buffer\n",
    "\n",
    "__Buffers:__ \n",
    "\n",
    "-> Buffers are used to store temporiarly data   \n",
    "-> Often used when there is in asymetry between input and output of data (e.g. Inputing and Image to a neural net [input] and outputing the classification result[output])  \n",
    "-> To high buffers sizes often lead to OUtOfMemory Errors  \n",
    "\n",
    "__```.shuffling()```__\n",
    "\n",
    "-> Will create a new dataset thats starts filling up a Buffer  \n",
    "-> Whenever it's asked to pull an item, it's feeding it from the buffer  \n",
    "-> Buffer filled up with new elements  \n",
    "-> Loop continues till StopIteration  \n",
    "\n",
    "\n",
    "__Large Dataset__  \n",
    "-> Maybe shuffling is not sufficient -> e.g. Buffer to small due to memory limitations  \n",
    "-> We could shuffle the dataset before  \n",
    "-> __Optimal:__ Divide the data in different files and feed them randomly during training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf084fe4-27e4-4ed1-b50b-d5c40e5296e8",
   "metadata": {},
   "outputs": [],
   "source": [
    " dataset = tf.data.Dataset.range(10).repeat(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4167409-d876-4662-8dfc-cfe000291369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 3 4 5 6 0 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 0 1 2 8 4 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6 8 9 0 9 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 4 1 6 5 8 7], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.shuffle(buffer_size=3, seed=2).batch(7, drop_remainder=True):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c59350-7906-463b-9ab6-98f862b0e4fe",
   "metadata": {},
   "source": [
    "## Interleaving\n",
    "\n",
    "The idea to divide the data set into different files and to process them incremently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "335055dd-38d9-4295-8e01-15210e9f3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fps = [\"../..data/housing_1.csv\", \"../../data/housing_2.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70863e0-a88c-4952-b507-03cec984197f",
   "metadata": {},
   "source": [
    "### ```.list_files()```\n",
    "\n",
    "Produces \"globbed\" file names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95d77503-d42d-41df-b8b1-d404da382968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset only containing file paths\n",
    "fp_dataset = tf.data.Dataset.list_files(train_fps, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defea424-d18f-45f2-a638-239b95a54ef0",
   "metadata": {},
   "source": [
    "### ```data.TextLineDataset()```\n",
    "\n",
    "Creates a dataset containing lines of text file. With CSV-Files we use ```.skip(1)``` to skip the csv header. Read Data are only byte string they need to __parsed!!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9531456-a1d7-4535-9be5-7d8fe8cf11e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8fbc8c28-9082-4aba-ad86-f1472d65d1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'-122.23,37.88,41.0,880.0,129.0,322.0,126.0,8.3252,452600.0,NEAR BAY', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in tf.data.TextLineDataset(\"../../data/housing_1.csv\").skip(1):\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ae881-c2b8-4bf5-823e-dc804976605f",
   "metadata": {},
   "source": [
    "### ```.interleave()```\n",
    "\n",
    "__General__: Execute a function on multiple files at one time, in random order.   \n",
    "__More specific__: Pulls from multiple files one line at the time in random order.   \n",
    "\n",
    "\n",
    "_num_parallel_cells:_ Argument to run interleave in parallel. This argument is later used to multithread the loading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35851704-b35e-4573-8593-1992de0ebc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = fp_dataset.interleave(\n",
    "    lambda file_path: tf.data.TextLineDataset(file_path).skip(1),\n",
    "    cycle_length = n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "547e06db-dcf6-480e-aa5e-abc6b0883302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'-122.23,37.88,41.0,880.0,129.0,322.0,126.0,8.3252,452600.0,NEAR BAY', shape=(), dtype=string)\n",
      "tf.Tensor(b'-122.22,37.86,21.0,7099.0,1106.0,2401.0,1138.0,8.3014,358500.0,NEAR BAY', shape=(), dtype=string)\n",
      "tf.Tensor(b'-122.24,37.85,52.0,1467.0,190.0,496.0,177.0,7.2574,352100.0,NEAR BAY', shape=(), dtype=string)\n",
      "tf.Tensor(b'-122.25,37.85,52.0,1274.0,235.0,558.0,219.0,5.6431,341300.0,NEAR BAY', shape=(), dtype=string)\n",
      "tf.Tensor(b'-122.25,37.85,52.0,1627.0,280.0,565.0,259.0,3.8462,342200.0,NEAR BAY', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab8257-132b-4f63-a291-deb69aefa4b2",
   "metadata": {},
   "source": [
    "# Prefetech & Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09046337-7aa3-4411-a4b6-5ce711440d6f",
   "metadata": {},
   "source": [
    "<img src=\"../../img/1302.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed5ecb-c63f-4d66-9967-fb17a42cc6f5",
   "metadata": {},
   "source": [
    "### ```.prefetch()```\n",
    "\n",
    "Tries to be one dataset on batch ahead. The Prefetch dataset will work in parallel to always try to keep a batch ready. \n",
    "\n",
    "Benefits: \n",
    "\n",
    "1. Performance\n",
    "2. Ensure that preprocccing and loading are multithreaded\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05f7f4-90ef-4fd3-a9be-6ae80f9e5830",
   "metadata": {},
   "source": [
    "<img src=\"../../img/1303.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8562ba4-7900-47ab-a91b-015259533f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
