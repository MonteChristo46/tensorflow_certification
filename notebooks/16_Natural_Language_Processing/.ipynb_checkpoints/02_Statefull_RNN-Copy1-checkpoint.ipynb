{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6749edea-792a-4a42-83f8-965cd82818f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8233512-a503-4d89-9274-d92ebeff766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\",shakespeare_url)\n",
    "with open(filepath) as file: \n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e93ac18f-be33-446c-87e6-14197a4cc58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:15] # Just a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e76ff04-58e9-4dab-a138-390e66b9018e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144c221-e2d3-40bc-a21c-1dee59a7d2db",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39ab3783-2ee7-4ad8-920f-48dbfb228947",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f6663-306c-4406-8699-9d7ad01e2195",
   "metadata": {},
   "source": [
    "## Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba58b454-f4f5-4cac-9cbc-09d45d6c2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) # Reading string on character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bccc8c86-a36e-4461-9e81-e3630ed451c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ab5f561-a05e-42e6-8caa-6224ad98045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "248f34fd-8707-4d2e-b90b-41fb4740ae0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  5,  8,  7,  2]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ids) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a883d590-1ed4-4efe-84d3-25e44620e7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e21ef-7151-4daf-adc0-c0f15942ed1e",
   "metadata": {},
   "source": [
    "## Creating the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b7143039-a69f-44eb-bb0e-b9a4a3432670",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([text]))-1 # Nice hack to make a ndarray with shape [1,N] to [,N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c9b1f83c-e633-49b2-b2f4-6e9e1c8b118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(tokenizer.document_count * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0a6b00c4-a5f9-4cc0-bf1f-bdd55a543fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size]) # Basically one list with containing train_size characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "55b97c45-d7a5-471d-9b91-104ba4bd6219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(19, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "    print(item) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f7509-4105-4057-a754-3814d9d3f48c",
   "metadata": {},
   "source": [
    "## Creating windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "837a3645-daa2-46a8-91a9-3ca73a03cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.window(n_steps + 1 ,shift = n_steps, drop_remainder=True) # To create non overlapping windows we use n_shift = n_steps; Should be some kind of nested list now. Each N_steps + 1 Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1133c19e-4921-49f1-a0a3-105af8bbb479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 6, 9, 8, 3, 1, 19, 6, 3, 6, 36, 2, 10, 24, 11, 22, 2, 20, 4, 9, 2, 1, 17, 2, 1, 23, 9, 4, 19, 2, 2, 13, 1, 5, 10, 16, 1, 20, 14, 9, 3, 7, 2, 9, 18, 1, 7, 2, 5, 9, 1, 15, 2, 1, 8, 23, 2, 5, 25, 27, 11, 11, 5, 12, 12, 24, 11, 8, 23, 2, 5, 25, 18, 1, 8, 23, 2, 5, 25, 27, 11, 11, 20, 6, 9, 8, 3, 1, 19, 6, 3, 6, 36, 2, 10, 24, 11, 16, 4, 14, 1]\n",
      "['f i r s t   c i t i z e n : \\n b e f o r e   w e   p r o c e e d   a n y   f u r t h e r ,   h e a r   m e   s p e a k . \\n \\n a l l : \\n s p e a k ,   s p e a k . \\n \\n f i r s t   c i t i z e n : \\n y o u  ']\n",
      "\n",
      "\n",
      "[1, 5, 9, 2, 1, 5, 12, 12, 1, 9, 2, 8, 4, 12, 26, 2, 13, 1, 9, 5, 3, 7, 2, 9, 1, 3, 4, 1, 13, 6, 2, 1, 3, 7, 5, 10, 1, 3, 4, 1, 20, 5, 15, 6, 8, 7, 30, 11, 11, 5, 12, 12, 24, 11, 9, 2, 8, 4, 12, 26, 2, 13, 27, 1, 9, 2, 8, 4, 12, 26, 2, 13, 27, 11, 11, 20, 6, 9, 8, 3, 1, 19, 6, 3, 6, 36, 2, 10, 24, 11, 20, 6, 9, 8, 3, 18, 1, 16, 4, 14, 1]\n",
      "['  a r e   a l l   r e s o l v e d   r a t h e r   t o   d i e   t h a n   t o   f a m i s h ? \\n \\n a l l : \\n r e s o l v e d .   r e s o l v e d . \\n \\n f i r s t   c i t i z e n : \\n f i r s t ,   y o u  ']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-17 20:17:24.561628: W tensorflow/core/framework/dataset.cc:679] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "for window in dataset.take(2):\n",
    "    tokens = [elem.numpy()+1 for elem in window]  # Remeber Windows are DatasetObjects by itself\n",
    "    print(tokens) #\n",
    "    print(tokenizer.sequences_to_texts([tokens]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e2360c41-0d10-449a-863c-0d01dc6d8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps+1)) # We made everything to a big list containing batches  that are equal to the windows. Just transforming a nested dataset to a dataset with batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fc6e6c15-e335-4b9a-a2b3-0fffb8acf834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1\n",
      "  0 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1\n",
      "  4  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24\n",
      " 17  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23\n",
      " 10 15  3 13  0], shape=(101,), dtype=int64)\n",
      "['f i r s t   c i t i z e n : \\n b e f o r e   w e   p r o c e e d   a n y   f u r t h e r ,   h e a r   m e   s p e a k . \\n \\n a l l : \\n s p e a k ,   s p e a k . \\n \\n f i r s t   c i t i z e n : \\n y o u  ']\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[ 0  4  8  1  0  4 11 11  0  8  1  7  3 11 25  1 12  0  8  4  2  6  1  8\n",
      "  0  2  3  0 12  5  1  0  2  6  4  9  0  2  3  0 19  4 14  5  7  6 29 10\n",
      " 10  4 11 11 23 10  8  1  7  3 11 25  1 12 26  0  8  1  7  3 11 25  1 12\n",
      " 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 19  5  8  7  2 17\n",
      "  0 15  3 13  0], shape=(101,), dtype=int64)\n",
      "['  a r e   a l l   r e s o l v e d   r a t h e r   t o   d i e   t h a n   t o   f a m i s h ? \\n \\n a l l : \\n r e s o l v e d .   r e s o l v e d . \\n \\n f i r s t   c i t i z e n : \\n f i r s t ,   y o u  ']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "    print(item)\n",
    "    print(tokenizer.sequences_to_texts([item.numpy()+1]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9f1d5834-7413-4bf6-a6e6-35cbf3245aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(1) # Just to give it a shape [1, Window_length]??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "edf52647-4791-45d3-b624-9ac6c0a2c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 101)\n"
     ]
    }
   ],
   "source": [
    "for x in dataset.take(1):\n",
    "    print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e672a28c-707b-4467-8afa-7de21c6eb734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nimm von EINEM WINDOW alle Buchstaben bis auf das letzte ( = Window_length  - 1) für X und für Y alle Buchstaben bis auf das erste (=Window_length - 1). \n",
    "#Darum ist die WIndow Length auch n-Steps +1, damit wir unser Y bilden können\n",
    "dataset = dataset.map(lambda windows: (windows[:,:-1], windows[:, 1:])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f1be4ef5-69f2-4a3b-aaa0-3c3d55dfdd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      ": ['f i r s t   c i t i z e n : \\n b e f o r e   w e   p r o c e e d   a n y   f u r t h e r ,   h e a r   m e   s p e a k . \\n \\n a l l : \\n s p e a k ,   s p e a k . \\n \\n f i r s t   c i t i z e n : \\n y o u'] \n",
      "\n",
      "Y: \n",
      ": ['i r s t   c i t i z e n : \\n b e f o r e   w e   p r o c e e d   a n y   f u r t h e r ,   h e a r   m e   s p e a k . \\n \\n a l l : \\n s p e a k ,   s p e a k . \\n \\n f i r s t   c i t i z e n : \\n y o u  '] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(1):\n",
    "    x_tokens = x.numpy()\n",
    "    y_tokens = y.numpy()\n",
    "    print(\"X: \\n: {} \\n\".format(tokenizer.sequences_to_texts(x_tokens+1)))\n",
    "    print(\"Y: \\n: {} \\n\".format(tokenizer.sequences_to_texts(y_tokens+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a7876827-e59e-4296-891d-6647f95d5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x, y: (tf.one_hot(x, depth=len(tokenizer.word_index)), y)) # Characters to one word vectors. Shape = [Batch_Size, N_Steps, Vocabulary_size]. N_Steps sind hier input words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "516fb920-a553-4385-809e-360aeafd8161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 100, 39), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(1, 100), dtype=int64, numpy=\n",
      "array([[ 5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1,\n",
      "        19,  3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,\n",
      "         4,  9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,\n",
      "         8,  0, 14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11,\n",
      "        23, 10,  7, 22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10,\n",
      "        10, 19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10,\n",
      "        15,  3, 13,  0]])>)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ab9a1ffc-bce7-4259-9d64-d6d17832800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdfc0e5-b73b-485c-b9fa-adea8142084d",
   "metadata": {},
   "source": [
    "## Building the model \n",
    "\n",
    "A Stateless model does basically never resets it's state. Neverless it's makes sense to reset the state after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "97ce9933-9dc9-4aff-91c6-771a8866b4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences = True, stateful = True, dropout = 0.2, recurrent_dropout = 0.2, batch_input_shape = [1, None, len(tokenizer.word_index)]),\n",
    "    keras.layers.GRU(128, return_sequences = True, stateful = True, dropout = 0.2, recurrent_dropout = 0.2, batch_input_shape = [1, None, len(tokenizer.word_index)]),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(len(tokenizer.word_index), activation = \"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d4dfb094-7eb3-4087-87e8-70463a15dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354ca714-b0e1-42e9-b8a7-e059fda46d21",
   "metadata": {},
   "source": [
    "## Compile & Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d0117fea-5969-411c-8722-11f4adb0f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir() -> str:\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H-%M-%S\")\n",
    "    return os.path.join(\".logs/\", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7e684399-1d84-4263-a94f-a5a823b0f4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-17 20:38:31.018068: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-17 20:38:31.018109: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-17 20:38:31.018889: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
      "2021-10-17 20:38:31.019242: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2021-10-17 20:38:31.128733: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n",
      "2021-10-17 20:38:31.128819: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(get_run_logdir(),  update_freq= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "60fd3d31-b33f-47ae-ac38-ef8501492568",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6dc178c9-e8b8-4fa7-a663-b732a2dcf2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "      1/Unknown - 3s 3s/step - loss: 2.0110"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-17 20:38:45.520468: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-17 20:38:45.520498: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-17 20:38:45.520981: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2/Unknown - 3s 459ms/step - loss: 2.1498"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-17 20:38:45.843236: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-10-17 20:38:45.996077: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2021-10-17 20:38:46.028691: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-10-17 20:38:46.052667: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: .logs/run_2021_10_17-20-38-31/train/plugins/profile/2021_10_17_20_38_46\n",
      "\n",
      "2021-10-17 20:38:46.057402: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to .logs/run_2021_10_17-20-38-31/train/plugins/profile/2021_10_17_20_38_46/daniel-ubuntu.trace.json.gz\n",
      "2021-10-17 20:38:46.204143: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: .logs/run_2021_10_17-20-38-31/train/plugins/profile/2021_10_17_20_38_46\n",
      "\n",
      "2021-10-17 20:38:46.214217: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to .logs/run_2021_10_17-20-38-31/train/plugins/profile/2021_10_17_20_38_46/daniel-ubuntu.memory_profile.json.gz\n",
      "2021-10-17 20:38:46.216503: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: .logs/run_2021_10_17-20-38-31/train/plugins/profile/2021_10_17_20_38_46\n",
      "Dumped tool data for xplane.pb to .logs/run_2021_10_17-20-38-31/train/plugins/profile/2021_10_17_20_38_46/daniel-ubuntu.xplane.pb\n",
      "Dumped tool data for overview_page.pb to .logs/run_2021_10_17-20-38-31/train/plugins/profile/2021_10_17_20_38_46/daniel-ubuntu.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to .logs/run_2021_10_17-20-38-31/train/plugins/profile/2021_10_17_20_38_46/daniel-ubuntu.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to .logs/run_2021_10_17-20-38-31/train/plugins/profile/2021_10_17_20_38_46/daniel-ubuntu.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to .logs/run_2021_10_17-20-38-31/train/plugins/profile/2021_10_17_20_38_46/daniel-ubuntu.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10038/10038 [==============================] - 2425s 241ms/step - loss: 2.3671\n",
      "Epoch 2/50\n",
      "10038/10038 [==============================] - 2346s 234ms/step - loss: 1.8364\n",
      "Epoch 3/50\n",
      " 9032/10038 [=========================>....] - ETA: 3:55 - loss: 1.7104"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14938/2802315173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mResetStatesCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(dataset, epochs = 50, callbacks = [ResetStatesCallback(), tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad55d8-d5cf-4f8c-8843-3e0f391147ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
