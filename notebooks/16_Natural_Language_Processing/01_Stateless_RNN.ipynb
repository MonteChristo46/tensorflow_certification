{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482a1139-8b5c-46fd-9b3c-082bc901be99",
   "metadata": {},
   "source": [
    "# Sequence to Sequence RNN (Stateless)\n",
    "- Random Portions of Text each Iteration\n",
    "- Without any informaiton about the rest of the text\n",
    "- Hidden State is not perserved between training iterations; Each Training Iteration the hidden state = 0; will be reseted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1988a-ecbb-4385-8f05-41d561760844",
   "metadata": {},
   "source": [
    "## Generating Shakespeare Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a19998-657c-418b-93a3-d8157a509973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982b636a-0844-4951-a0b6-ba070edf4fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\",shakespeare_url)\n",
    "with open(filepath) as file: \n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203cdb89-3c65-47ae-a906-39927a7a5299",
   "metadata": {},
   "source": [
    "## Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6949db95-b738-4f25-ae65-4b45db181d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dffa53-0fd7-4d76-a7ea-b5accc5b6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae0a5e2-e6d2-4e39-9448-66c7154d77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a8e9a9-b94c-4dad-bdbd-a513f9476fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c67d96f-971b-4f0e-a026-9510ff66eda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9adca27-7b29-4b1a-bb5e-bf4c9cdb300d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ' ',\n",
       " 2: 'e',\n",
       " 3: 't',\n",
       " 4: 'o',\n",
       " 5: 'a',\n",
       " 6: 'i',\n",
       " 7: 'h',\n",
       " 8: 's',\n",
       " 9: 'r',\n",
       " 10: 'n',\n",
       " 11: '\\n',\n",
       " 12: 'l',\n",
       " 13: 'd',\n",
       " 14: 'u',\n",
       " 15: 'm',\n",
       " 16: 'y',\n",
       " 17: 'w',\n",
       " 18: ',',\n",
       " 19: 'c',\n",
       " 20: 'f',\n",
       " 21: 'g',\n",
       " 22: 'b',\n",
       " 23: 'p',\n",
       " 24: ':',\n",
       " 25: 'k',\n",
       " 26: 'v',\n",
       " 27: '.',\n",
       " 28: \"'\",\n",
       " 29: ';',\n",
       " 30: '?',\n",
       " 31: '!',\n",
       " 32: '-',\n",
       " 33: 'j',\n",
       " 34: 'q',\n",
       " 35: 'x',\n",
       " 36: 'z',\n",
       " 37: '3',\n",
       " 38: '&',\n",
       " 39: '$'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word # We can see that the tokenizer starts with one - that's stupid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b154dfee-a41f-4a07-a5c4-27cacedac85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of that we need to substract -1\n",
    "encoded_vocabulary = np.array(tokenizer.texts_to_sequences([text])) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c23426-3afd-485f-b358-4925628fc756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1115394)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_vocabulary.shape # We want a simple list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5609335d-e75a-42a6-9965-46b1e7588faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded_vocabulary] = encoded_vocabulary # Whats happening here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff122625-d599-4ecf-93fa-404952330c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115394,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_vocabulary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d36436-af7b-4101-9e3e-a80dd93107e2",
   "metadata": {},
   "source": [
    "## How to split text dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b23d1-9e2f-4fde-8b8d-03b6696866c0",
   "metadata": {},
   "source": [
    "- Its important to avoid overlaps betwenn training, validation and test_set\n",
    "\n",
    "1. Create TF.DataSets!\n",
    "2. Build NestedMaps/Sequences/WindowDatasets\n",
    "3. Network can only train with Tensors! A FlatMapDatasets is a dataset that contain tensors not nested datasets! --> use ```dataset.flat_map()```\n",
    "4. At the same time we build we use the batch function to resample the split in windows again! It's basically just a parsing of objects because models can't deal with nested datasets --> ```dataset.flat_map(lambda x: x.batch(2))```; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42962f29-cadb-45df-9cdf-f7ce1cfc9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = tokenizer.document_count * 90 // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b9604c-13c4-432a-b50b-cb7f16618248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-18 14:10:29.881690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-18 14:10:29.887756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-18 14:10:29.888259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-18 14:10:29.889530: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-18 14:10:29.890207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-18 14:10:29.890765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-18 14:10:29.891258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-18 14:10:30.277996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-18 14:10:30.278490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-18 14:10:30.278947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-18 14:10:30.279417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6040 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:0b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded_vocabulary[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ecb44a-9377-47df-ac77-e3fadeb2be12",
   "metadata": {},
   "source": [
    "### Creating Sequences (We wanna feed sequences into our RNN )\n",
    "\n",
    "- Important! Those are not equal to batches. A batch can contain multiple sequences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b8aeb2-7045-4fa3-81fd-4957d1c74c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # This is somehow important - i dont understand why?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f213d6b-9d6f-48f5-93a3-8fd0b0836766",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.window(window_length, drop_remainder=True, shift=1) # One window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "098de4d9-e0fb-46e3-83e3-0f5c23409603",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda x: x.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb0d9375-9fdd-4a45-a4ee-36ffb4615bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: (None,), types: tf.int64>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc07dec-6f80-40c5-89e5-7723ece6efa7",
   "metadata": {},
   "source": [
    "### Shuffling (Gradient Decent works best with independet instances)\n",
    "\n",
    "1. Remember when we train we need a X and Y (obvisously)\n",
    "2. We predict on character level\n",
    "3. Each window has for every character one steps (neuron) - each neuron is trying to predict the next character based on it's input. (And hidden state..)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd514222-0a02-4920-a9e9-96bbc741d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a2dfb89-3a14-4767-98f1-099339b98be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a1cf8c1-d199-410e-a7dc-10b7f767587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: (x[:, :-1], x[:,1:])) # First is X  - everything till the last character, Second is Y - everything except the first element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1e8cf1f-a053-4587-9501-68ae8127089c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None, None), (None, None)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ba212-a34e-4607-85a2-b4e4247ccab3",
   "metadata": {},
   "source": [
    "### Creating One Hot Vectors for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b870074-ddd1-4244-91de-d7d50baf724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x, y: (tf.one_hot(x, depth=len(tokenizer.word_index)), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8614c000-cabb-42ed-9890-b9e823c389e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None, None, 39), (None, None)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7afd22d-e5a9-4203-a8a9-61e9dddd5e7f",
   "metadata": {},
   "source": [
    "### Prefetching\n",
    "\n",
    "Be one dataset ahead of batch - parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c4043-31ee-47cb-b77a-b7006b496124",
   "metadata": {},
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de018f8d-0d7a-4e31-974d-a9a76c45e151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None, None, 39), (None, None)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b5cfd78-6bdd-498c-b9ea-732eacc9b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-18 14:10:30.513569: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44a147-ede2-4ac2-9652-be0b1e123a1b",
   "metadata": {},
   "source": [
    "## Creating the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c7423d4-6b0f-44aa-b440-6ad612d9110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True, input_shape = [None, len(tokenizer.word_index)], dropout=0.2), \n",
    "    keras.layers.LSTM(128, return_sequences=True, dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(len(tokenizer.word_index), activation=\"softmax\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5171ad3-23f3-432d-979c-cf15654a1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(), loss = keras.losses.sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55029bcd-1a50-4be4-8370-e9e300171c85",
   "metadata": {},
   "source": [
    "### Setting up TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99a7d9b2-1783-4a3e-b0d1-7f175fa7bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6015aad-867a-4883-b182-4b97ff068b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2eda7e95-a8a0-4991-8d97-adbe0c7a647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir() -> str:\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H-%M-%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9290a037-6bb6-4aad-9869-eb33fad0d68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/run_2021_09_18-14-10-31'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e49d748a-cbbc-4c61-8bef-625e252a3e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-18 14:10:31.100045: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-09-18 14:10:31.100076: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-09-18 14:10:31.100123: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
      "2021-09-18 14:10:31.100372: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2021-09-18 14:10:31.201221: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n",
      "2021-09-18 14:10:31.201297: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir(),  update_freq= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07eb6807-b8f4-4bda-8074-05b48d47ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"shakespeare_text.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc5c8e-e2cd-4bde-b4bf-00234142446f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b5e4b7a-42d3-4222-9e48-b59cfad3afd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-18 14:10:33.865425: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8/Unknown - 3s 37ms/step - loss: 3.5481"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-18 14:10:34.630106: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-09-18 14:10:34.630143: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-09-18 14:10:34.630671: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n",
      "2021-09-18 14:10:34.651555: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-09-18 14:10:34.655512: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2021-09-18 14:10:34.656923: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-09-18 14:10:34.659478: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/run_2021_09_18-14-10-31/train/plugins/profile/2021_09_18_14_10_34\n",
      "\n",
      "2021-09-18 14:10:34.660323: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/run_2021_09_18-14-10-31/train/plugins/profile/2021_09_18_14_10_34/daniel-ubuntu.trace.json.gz\n",
      "2021-09-18 14:10:34.670556: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/run_2021_09_18-14-10-31/train/plugins/profile/2021_09_18_14_10_34\n",
      "\n",
      "2021-09-18 14:10:34.672786: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/run_2021_09_18-14-10-31/train/plugins/profile/2021_09_18_14_10_34/daniel-ubuntu.memory_profile.json.gz\n",
      "2021-09-18 14:10:34.672980: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/run_2021_09_18-14-10-31/train/plugins/profile/2021_09_18_14_10_34\n",
      "Dumped tool data for xplane.pb to ./logs/run_2021_09_18-14-10-31/train/plugins/profile/2021_09_18_14_10_34/daniel-ubuntu.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/run_2021_09_18-14-10-31/train/plugins/profile/2021_09_18_14_10_34/daniel-ubuntu.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/run_2021_09_18-14-10-31/train/plugins/profile/2021_09_18_14_10_34/daniel-ubuntu.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/run_2021_09_18-14-10-31/train/plugins/profile/2021_09_18_14_10_34/daniel-ubuntu.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/run_2021_09_18-14-10-31/train/plugins/profile/2021_09_18_14_10_34/daniel-ubuntu.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31368/31368 [==============================] - 494s 16ms/step - loss: 1.6561\n",
      "Epoch 2/5\n",
      "31368/31368 [==============================] - 491s 16ms/step - loss: 1.5592\n",
      "Epoch 3/5\n",
      "31368/31368 [==============================] - 489s 16ms/step - loss: 1.5380\n",
      "Epoch 4/5\n",
      "31368/31368 [==============================] - 489s 16ms/step - loss: 1.5270\n",
      "Epoch 5/5\n",
      "31368/31368 [==============================] - 489s 16ms/step - loss: 1.5210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6bc0505eb0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=5, callbacks = [tensorboard_cb, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ada79c-fd12-4693-a150-0b881a3a9875",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d69e22b8-ea73-4073-b5db-5c03d84c5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1 # Tokenizer starts at 1\n",
    "    return tf.one_hot(X, len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b42298c-9a25-4ec2-beb1-07e71ebce447",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess([\"Romeo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "baab978a-225c-4868-9a2f-15703535ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model(X_new), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18352dd1-e528-4b6c-97dd-fb670e6941b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(y_pred+1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9069ae6-d70c-47f1-8dd4-e93d2c5f5dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
